{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf10 Home! Lets Start","text":"\ud83d\udcde Contact Me <p> Email: karti479@gmail.com |      Phone: +91-8742983860 GitHub: GitHub Profile |      LinkedIn: LinkedIn Profile"},{"location":"#auto-hpa","title":"Auto-HPA","text":"<ul> <li>The Auto-HPA Metrics Helm Chart provides a production-ready solution to </li> <li>dynamically scale Kubernetes workloads using custom metrics, CPU/memory metrics, and external metrics. </li> <li>It integrates seamlessly with Prometheus, Prometheus Adapter, and Kubernetes(Horizontal Pod Autoscaler).</li> </ul>"},{"location":"#repo","title":"Repo","text":"<pre><code>https://github.com/karti479/auto-hpa.git\n</code></pre>"},{"location":"#1-overview","title":"1. Overview","text":"<p>The Auto-Scale Metrics Helm Chart provides a production-grade solution for scaling Kubernetes workloads dynamically using:</p> <ul> <li>Kubernetes HPA (Horizontal Pod Autoscaler)</li> <li>Prometheus Adapter for custom metrics</li> <li>Prometheus for metrics collection and monitoring</li> </ul> <p>It supports scaling based on:</p> <ul> <li>CPU/Memory Utilization</li> <li>Custom Metrics like request rates, latency, or error rates</li> <li>External Metrics such as job queues or user-defined indicators</li> </ul> <p>The chart is highly configurable and integrates modularly with existing Prometheus setups.</p>"},{"location":"#2-features","title":"2. Features","text":"Feature Description Dynamic Scaling Scale workloads dynamically based on CPU, memory, and custom metrics. Custom Metrics Support Uses Prometheus Adapter for translating metrics for HPA. Health Monitoring Liveness and readiness probes for application, Prometheus, and adapter. RBAC Integration Ensures secure access to Kubernetes API and custom metrics endpoints. Modular Design Deploy Prometheus, Adapter, and HPA independently or together. Production-Ready Namespace isolation, robust health checks, and resource configurations. Scalable Supports multi-metric scaling and advanced configurations."},{"location":"#3-architecture-and-components","title":"3. Architecture and Components","text":"<p>The Helm chart consists of the following key components:</p>"},{"location":"#31-application-deployment","title":"3.1 Application Deployment","text":"<ul> <li>Deploys the user application</li> <li>Exposes a /metrics endpoint compatible with Prometheus</li> </ul>"},{"location":"#32-prometheus","title":"3.2 Prometheus","text":"<ul> <li>Scrapes metrics from the /metrics endpoint</li> <li>Provides a centralized metrics store for Kubernetes HPA</li> </ul>"},{"location":"#33-prometheus-adapter","title":"3.3 Prometheus Adapter","text":"<ul> <li>Translates Prometheus metrics into Kubernetes Custom Metrics API</li> <li>Enables Kubernetes HPA to use custom metrics for scaling</li> </ul>"},{"location":"#34-hpa-horizontal-pod-autoscaler","title":"3.4 HPA (Horizontal Pod Autoscaler)","text":"<ul> <li>Queries Kubernetes Metrics API to scale pods dynamically</li> </ul>"},{"location":"#4-installation-guide","title":"4. Installation Guide","text":""},{"location":"#41-prerequisites","title":"4.1 Prerequisites","text":"<p>Ensure the following are available: 1. Kubernetes Cluster (v1.24+) 2. Helm (v3+) 3. Prometheus-compatible Application exposing metrics at /metrics</p>"},{"location":"#42-steps-for-installation","title":"4.2 Steps for Installation","text":"<p>Step 1: Clone the Helm Chart Repository</p> <pre><code>git clone https://github.com/karti479/auto-hpa.git\ncd auto-hpa\n</code></pre> <p>Step 2: Configure values.yaml Customize your values.yaml file as per your requirements (refer to section 5. User Configuration Guide).</p> <p>Step 3: Install the Helm Chart <pre><code>helm install auto-scale ./auto-scale-metrics -f values.yaml\n</code></pre></p> <p>Step 4: Verify Installation - Check all resources: <pre><code>kubectl get all -n &lt;namespace&gt;\n</code></pre> - Validate the Prometheus and Adapter pods: <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"#5-user-configuration-guide","title":"5. User Configuration Guide","text":"<p>This section provides detailed instructions and examples for configuring the Helm chart via values.yaml.</p>"},{"location":"#51-general-application-configuration","title":"5.1 General Application Configuration","text":"<pre><code>app:\n  name: auto-scale-app\n  namespace: default\n  image:\n    repository: my-docker-repo/auto-scale-app\n    tag: v1.0.0\n    pullPolicy: IfNotPresent\n  port: 8080\n  metricsPath: /metrics\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n    limits:\n      cpu: 500m\n      memory: 512Mi\n</code></pre>"},{"location":"#52-prometheus-configuration","title":"5.2 Prometheus Configuration","text":"<pre><code>prometheus:\n  enabled: true\n  scrapeInterval: 15s\n  scrapeConfigs:\n    - job_name: 'auto-scale-app'\n      static_configs:\n        - targets: ['auto-scale-app:8080']\n</code></pre>"},{"location":"#53-prometheus-adapter-configuration","title":"5.3 Prometheus Adapter Configuration","text":"<pre><code>prometheusAdapter:\n  enabled: true\n  rules:\n    - seriesQuery: 'http_requests_total{namespace!=\"\",pod!=\"\"}'\n      resources:\n        overrides:\n          namespace: {resource: \"namespace\"}\n          pod: {resource: \"pod\"}\n      name:\n        matches: \"^(.*)_total\"\n        as: \"${1}_per_second\"\n      metricsQuery: 'rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[2m])'\n</code></pre>"},{"location":"#54-hpa-configuration","title":"5.4 HPA Configuration","text":"<pre><code>hpa:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 80\n    - type: Pods\n      pods:\n        metric:\n          name: http_requests_per_second\n        target:\n          type: AverageValue\n          averageValue: 10\n</code></pre>"},{"location":"#6-metrics-scraping-and-handling","title":"6. Metrics Scraping and Handling","text":""},{"location":"#61-metrics-exposure","title":"6.1 Metrics Exposure","text":"<p>Python Example:</p> <pre><code>from prometheus_client import start_http_server, Counter\nimport time\n\nREQUESTS = Counter('http_requests_total', 'Total HTTP Requests')\n\ndef process_request():\n    REQUESTS.inc()\n\nif __name__ == '__main__':\n    start_http_server(8000)\n    while True:\n        process_request()\n        time.sleep(1)\n</code></pre> <p>Node.js Example:</p> <pre><code>const express = require('express');\nconst promClient = require('prom-client');\n\nconst app = express();\nconst collectDefaultMetrics = promClient.collectDefaultMetrics;\ncollectDefaultMetrics({ timeout: 5000 });\n\nconst httpRequestsTotal = new promClient.Counter({\n    name: 'http_requests_total',\n    help: 'Total number of HTTP requests'\n});\n\napp.get('/', (req, res) =&gt; {\n    httpRequestsTotal.inc();\n    res.send('Hello World!');\n});\n\napp.get('/metrics', async (req, res) =&gt; {\n    res.set('Content-Type', promClient.register.contentType);\n    res.end(await promClient.register.metrics());\n});\n\napp.listen(8080, () =&gt; console.log('Server running on port 8080'));\n</code></pre>"},{"location":"#7-health-monitoring","title":"7. Health Monitoring","text":"<pre><code>healthChecks:\n  prometheus:\n    liveness:\n      httpGet:\n        path: /-/healthy\n        port: 9090\n    readiness:\n      httpGet:\n        path: /-/ready\n        port: 9090\n  prometheusAdapter:\n    liveness:\n      httpGet:\n        path: /healthz\n        port: 6443\n    readiness:\n      httpGet:\n        path: /healthz\n        port: 6443\n  app:\n    liveness:\n      httpGet:\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 10\n      periodSeconds: 5\n    readiness:\n      httpGet:\n        path: /ready\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 10\n</code></pre>"},{"location":"#8-product-workflow","title":"8. Product Workflow","text":"<ol> <li>Application exposes metrics</li> <li>Prometheus scrapes metrics</li> <li>Prometheus Adapter translates metrics</li> <li>HPA queries Custom Metrics API</li> <li>HPA makes scaling decisions</li> <li>Kubernetes scales the deployment</li> </ol>"},{"location":"#9-use-cases","title":"9. Use Cases","text":""},{"location":"#91-cpu-and-memory-scaling","title":"9.1 CPU and Memory Scaling","text":"<pre><code>hpa:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 80\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n</code></pre>"},{"location":"#92-scaling-based-on-requests-per-second","title":"9.2 Scaling Based on Requests Per Second","text":"<pre><code>hpa:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n    - type: Pods\n      pods:\n        metric:\n          name: http_requests_per_second\n        target:\n          type: AverageValue\n          averageValue: 50\n</code></pre>"},{"location":"#93-scaling-based-on-queue-length","title":"9.3 Scaling Based on Queue Length","text":"<pre><code>hpa:\n  enabled: true\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n    - type: External\n      external:\n        metric:\n          name: queue_messages_ready\n          selector:\n            matchLabels:\n              queue: \"worker-jobs\"\n        target:\n          type: AverageValue\n          averageValue: 30\n</code></pre>"},{"location":"#10-troubleshooting-guide","title":"10. Troubleshooting Guide","text":""},{"location":"#101-hpa-not-scaling","title":"10.1 HPA Not Scaling","text":"<ol> <li> <p>Check HPA status: <pre><code>   kubectl describe hpa &lt;hpa-name&gt; -n &lt;namespace&gt;\n</code></pre></p> </li> <li> <p>Verify metrics:  <code>yaml    kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second\" | jq .</code></p> </li> <li> <p>Check Prometheus Adapter logs: <pre><code>   kubectl logs -l app=prometheus-adapter -n &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"#102-prometheus-not-scraping-metrics","title":"10.2 Prometheus Not Scraping Metrics","text":"<ol> <li> <p>Check Prometheus targets: <pre><code>   kubectl port-forward svc/prometheus 9090:9090 -n &lt;namespace&gt;\n</code></pre> Then visit http://localhost:9090/targets</p> </li> <li> <p>Verify scrape config: <pre><code>   kubectl get configmap prometheus-server -n &lt;namespace&gt; -o yaml\n</code></pre></p> </li> </ol>"},{"location":"#103-application-not-exposing-metrics","title":"10.3 Application Not Exposing Metrics","text":"<ol> <li> <p>Check if metrics endpoint is accessible: <pre><code>   kubectl port-forward svc/&lt;app-service&gt; 8080:8080 -n &lt;namespace&gt;\n   curl http://localhost:8080/metrics\n</code></pre></p> </li> <li> <p>Verify application logs: <pre><code>   kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"#11-pricing-and-licensing","title":"11. Pricing and Licensing","text":"<ul> <li>Free Usage: Helm chart available for free.</li> <li>Annual Support Licensing: \u20b91,000 INR/year.</li> <li>Includes email support and regular updates.</li> <li>karti479@gmail.com/ https://www.linkedin.com/in/product-kartik/</li> </ul>"},{"location":"#12-best-practices","title":"12. Best Practices","text":"<ol> <li>Set appropriate resource requests and limits for all components.</li> <li>Use namespaces to isolate the auto-scaling setup.</li> <li>Regularly update the Helm chart and its dependencies.</li> <li>Monitor and alert on the health of Prometheus and Prometheus Adapter.</li> <li>Test scaling behavior in a non-production environment before deploying to production.</li> </ol>"},{"location":"#13-conclusion","title":"13. Conclusion","text":"<p>The Auto-Scale Metrics Helm Chart provides a robust, flexible solution for implementing custom metrics-based autoscaling in Kubernetes environments. By leveraging Prometheus and the Prometheus Adapter, it enables fine-grained control over scaling decisions based on application-specific metrics.</p>"},{"location":"#14-extended-user-guide","title":"14. Extended User Guide","text":""},{"location":"#141-advanced-prometheus-adapter-configuration","title":"14.1 Advanced Prometheus Adapter Configuration","text":"<pre><code>prometheusAdapter:\n  rules:\n    - seriesQuery: '{__name__=~\"^container_.*\",container!=\"POD\",namespace!=\"\",pod!=\"\"}'\n      seriesFilters: []\n      resources:\n        overrides:\n          namespace:\n            resource: namespace\n          pod:\n            resource: pod\n      name:\n        matches: ^container_(.*)_seconds_total$\n        as: \"${1}_per_second\"\n      metricsQuery: sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[1m])) by (&lt;&lt;.GroupBy&gt;&gt;)\n</code></pre>"},{"location":"#142-implementing-custom-metrics","title":"14.2 Implementing Custom Metrics","text":"<ol> <li>Define the metric in your application</li> <li>Expose the metric via the /metrics endpoint</li> <li>Configure Prometheus to scrape the metric</li> <li>Set up Prometheus Adapter rules to make the metric available to Kubernetes</li> <li>Configure HPA to use the custom metric</li> </ol>"},{"location":"#example-workflow-for-a-queue_depth-metric","title":"Example workflow for a 'queue_depth' metric:","text":"<ol> <li>Application code (Python):</li> </ol> <pre><code>   from prometheus_client import Gauge\n\n   QUEUE_DEPTH = Gauge('queue_depth', 'Number of items in the queue')\n\n   def process_queue():\n       depth = get_queue_depth()  # Your queue depth logic here\n       QUEUE_DEPTH.set(depth)\n</code></pre> <ol> <li>Prometheus scrape config:</li> </ol> <pre><code>   scrape_configs:\n     - job_name: 'queue-app'\n       static_configs:\n         - targets: ['queue-app:8080']\n</code></pre> <ol> <li>Prometheus Adapter rule:</li> </ol> <pre><code>   rules:\n     - seriesQuery: 'queue_depth'\n       resources:\n         overrides:\n           namespace: {resource: \"namespace\"}\n           pod: {resource: \"pod\"}\n       name:\n         matches: \"^(.*)$\"\n         as: \"${1}\"\n       metricsQuery: 'avg(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;})'\n</code></pre> <ol> <li>HPA configuration:</li> </ol> <pre><code>   hpa:\n     metrics:\n       - type: Pods\n         pods:\n           metric:\n             name: queue_depth\n           target:\n             type: AverageValue\n             averageValue: 100\n</code></pre> <p>This extended guide provides more in-depth examples and configurations to help users implement advanced autoscaling scenarios using the Auto-Scale Metrics Helm Chart.</p> \ud83d\udcde Contact Me <p> Email: karti479@gmail.com |      Phone: +91-8742983860 GitHub: GitHub Profile |      LinkedIn: LinkedIn Profile"},{"location":"About/","title":"\u2699\ufe0f About Product","text":"\ud83d\udcde Contact Me <p> Email: karti479@gmail.com |      Phone: +91-8742983860 GitHub: GitHub Profile |      LinkedIn: LinkedIn Profile"},{"location":"About/#what-is-this-product","title":"What is this Product?","text":"<p>This product is an Auto-Scaling Solution implemented via a Helm Chart for Kubernetes. It integrates with Prometheus and Kubernetes Horizontal Pod Autoscaler (HPA) to provide automatic, application-aware scaling of your workloads based on metrics.</p>"},{"location":"About/#the-pain-point-it-solves","title":"The Pain Point It Solves","text":""},{"location":"About/#traditional-kubernetes-hpa-limitation","title":"Traditional Kubernetes HPA Limitation \ud83d\uded1:","text":"<p>Kubernetes Horizontal Pod Autoscaler (HPA) natively scales based on CPU and Memory usage only. However, modern applications often require scaling based on:</p> <ul> <li>\ud83d\udcca Requests Per Second (RPS)</li> <li>\u23f1\ufe0f Latency (e.g., P95, P99)</li> <li>\ud83d\udce5 Queue Backlog or Job Depth</li> </ul>"},{"location":"About/#the-solution-auto-scaling-helm-chart","title":"The Solution: Auto-Scaling Helm Chart","text":"<p>This product provides a Helm Chart that integrates the following components to solve this limitation:</p>"},{"location":"About/#custom-metrics-integration","title":"Custom Metrics Integration:","text":"<ul> <li>The product integrates with Prometheus and Prometheus Adapter to fetch custom application metrics.</li> <li>These metrics (e.g., requests per second, latency, queue depth) are exposed by the application and scraped by Prometheus.</li> </ul>"},{"location":"About/#automatic-hpa-configuration","title":"Automatic HPA Configuration:","text":"<p>The Helm Chart configures Kubernetes HPA to use: - Resource Metrics (CPU/Memory). - Custom Metrics (application-specific metrics). - External Metrics (e.g., message queue depth).</p>"},{"location":"About/#production-ready-features","title":"Production-Ready Features:","text":"<ul> <li>RBAC Support: Ensures Prometheus Adapter and HPA have proper permissions.</li> <li>Health Checks: Ensures Prometheus, Prometheus Adapter, and the application are healthy.</li> <li>Modular Deployment: Allows integration with existing Prometheus setups or installs new Prometheus if needed.</li> <li>Scalability: Handles scaling of multiple workloads across namespaces.</li> </ul>"},{"location":"About/#key-pain-points-solved","title":"Key Pain Points Solved","text":""},{"location":"About/#scaling-beyond-cpumemory","title":"Scaling Beyond CPU/Memory:","text":"<p>Applications can now scale based on: - Requests per second. - Latency (e.g., P99). - Queue length or backlog.</p>"},{"location":"About/#simplified-deployment","title":"Simplified Deployment:","text":"<p>The Helm Chart provides an automated, production-ready setup: - Install Prometheus and Prometheus Adapter (if not already available). - Configure Kubernetes HPA to leverage both default and custom metrics.</p>"},{"location":"About/#improved-application-performance","title":"Improved Application Performance:","text":"<p>By scaling based on real application behavior (e.g., high latency or load), the product ensures better performance and availability.</p>"},{"location":"About/#flexibility-for-any-application","title":"Flexibility for Any Application:","text":"<p>Works with any application that exposes metrics in Prometheus format on a <code>/metrics</code> endpoint.</p>"},{"location":"About/#time-and-cost-savings","title":"Time and Cost Savings:","text":"<ul> <li>Automates the manual scaling of applications.</li> <li>Optimizes resource utilization, reducing cloud infrastructure costs.</li> </ul>"},{"location":"About/#who-needs-this-product","title":"Who Needs This Product?","text":""},{"location":"About/#developers-devops-teams","title":"Developers &amp; DevOps Teams:","text":"<p>Looking for flexible scaling of applications in Kubernetes based on metrics like request load, queue size, or latency.</p>"},{"location":"About/#businesses-running-critical-applications","title":"Businesses Running Critical Applications:","text":"<p>Ensures the application scales quickly and efficiently during high demand, preventing downtime or degraded performance.</p>"},{"location":"About/#organizations-using-microservices","title":"Organizations Using Microservices:","text":"<p>Microservices often rely on multiple metrics (like request rate and latency) for scaling decisions. This product simplifies scaling for these architectures.</p>"},{"location":"About/#in-summary","title":"In Summary","text":"<p>This product addresses the gaps in Kubernetes HPA by enabling application-aware scaling with custom and external metrics. It automates the setup with a Helm Chart, reducing operational overhead and ensuring better resource utilization, cost-efficiency, and performance for Kubernetes workloads. \ud83d\ude80</p>"},{"location":"configuration/","title":"User Configuration Guide","text":"<p>This section provides detailed instructions and examples for configuring the Helm chart via values.yaml.</p>"},{"location":"configuration/#1-general-application-configuration","title":"1 General Application Configuration","text":"<pre><code>app:\n  name: auto-scale-app\n  namespace: default\n  image:\n    repository: my-docker-repo/auto-scale-app\n    tag: v1.0.0\n    pullPolicy: IfNotPresent\n  port: 8080\n  metricsPath: /metrics\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n    limits:\n      cpu: 500m\n      memory: 512Mi\n</code></pre>"},{"location":"configuration/#2-prometheus-configuration","title":"2 Prometheus Configuration","text":"<pre><code>prometheus:\n  enabled: true\n  scrapeInterval: 15s\n  scrapeConfigs:\n    - job_name: 'auto-scale-app'\n      static_configs:\n        - targets: ['auto-scale-app:8080']\n</code></pre>"},{"location":"configuration/#3-prometheus-adapter-configuration","title":"3 Prometheus Adapter Configuration","text":"<pre><code>prometheusAdapter:\n  enabled: true\n  rules:\n    - seriesQuery: 'http_requests_total{namespace!=\"\",pod!=\"\"}'\n      resources:\n        overrides:\n          namespace: {resource: \"namespace\"}\n          pod: {resource: \"pod\"}\n      name:\n        matches: \"^(.*)_total\"\n        as: \"${1}_per_second\"\n      metricsQuery: 'rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[2m])'\n</code></pre>"},{"location":"configuration/#4-hpa-configuration","title":"4 HPA Configuration","text":"<pre><code>hpa:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 80\n    - type: Pods\n      pods:\n        metric:\n          name: http_requests_per_second\n        target:\n          type: AverageValue\n          averageValue: 10\n</code></pre>"},{"location":"configuration/#6-metrics-scraping-and-handling","title":"6. Metrics Scraping and Handling","text":""},{"location":"configuration/#61-metrics-exposure","title":"6.1 Metrics Exposure","text":"<p>Python Example:</p> <pre><code>from prometheus_client import start_http_server, Counter\nimport time\n\nREQUESTS = Counter('http_requests_total', 'Total HTTP Requests')\n\ndef process_request():\n    REQUESTS.inc()\n\nif __name__ == '__main__':\n    start_http_server(8000)\n    while True:\n        process_request()\n        time.sleep(1)\n</code></pre> <p>Node.js Example:</p> <pre><code>const express = require('express');\nconst promClient = require('prom-client');\n\nconst app = express();\nconst collectDefaultMetrics = promClient.collectDefaultMetrics;\ncollectDefaultMetrics({ timeout: 5000 });\n\nconst httpRequestsTotal = new promClient.Counter({\n    name: 'http_requests_total',\n    help: 'Total number of HTTP requests'\n});\n\napp.get('/', (req, res) =&gt; {\n    httpRequestsTotal.inc();\n    res.send('Hello World!');\n});\n\napp.get('/metrics', async (req, res) =&gt; {\n    res.set('Content-Type', promClient.register.contentType);\n    res.end(await promClient.register.metrics());\n});\n\napp.listen(8080, () =&gt; console.log('Server running on port 8080'));\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#connect-author-for-support","title":"Connect Author for support","text":"<ul> <li>karti479@gmail.com</li> <li>https://www.linkedin.com/in/product-kartik/</li> </ul>"},{"location":"installation/#41-prerequisites","title":"4.1 Prerequisites","text":"<p>Ensure the following are available: 1. Kubernetes Cluster (v1.24+). 2. Helm (v3+). 3. Prometheus-compatible Application exposing metrics at <code>/metrics</code>.</p>"},{"location":"installation/#42-steps-for-installation","title":"4.2 Steps for Installation","text":"<p>Step 1: Clone the Helm Chart Repository</p> <pre><code>git clone https://github.com/karti479/auto-hpa.git\ncd auto-scale-metrics\n</code></pre> <p>Step 2: Configure values.yaml Customize your values.yaml file as per your requirements (refer to section 5. User Configuration Guide).</p> <p>Step 3: Install the Helm Chart</p> <pre><code>helm install auto-scale ./auto-scale-metrics -f values.yaml\n</code></pre> <p>Step 4: Verify Installation</p> <pre><code>kubectl get all -n &lt;namespace&gt;\nkubectl get pods -n &lt;namespace&gt;\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"troubleshooting/#1-hpa-not-scaling","title":"1 HPA Not Scaling","text":"<ol> <li>Check HPA status:</li> </ol> <pre><code>   kubectl describe hpa &lt;hpa-name&gt; -n &lt;namespace&gt;\n</code></pre> <ol> <li>Verify metrics:</li> </ol> <pre><code>   kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second\" | jq .\n</code></pre> <ol> <li>Check Prometheus Adapter logs: <pre><code>   kubectl logs -l app=prometheus-adapter -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"troubleshooting/#2-prometheus-not-scraping-metrics","title":"2 Prometheus Not Scraping Metrics","text":"<ol> <li>Check Prometheus targets:</li> </ol> <p><pre><code>   kubectl port-forward svc/prometheus 9090:9090 -n &lt;namespace&gt;\n</code></pre> Then visit http://localhost:9090/targets</p> <ol> <li>Verify scrape config: <pre><code>   kubectl get configmap prometheus-server -n &lt;namespace&gt; -o yaml\n</code></pre></li> </ol>"},{"location":"troubleshooting/#3-application-not-exposing-metrics","title":".3 Application Not Exposing Metrics","text":"<ol> <li> <p>Check if metrics endpoint is accessible: <pre><code>   kubectl port-forward svc/&lt;app-service&gt; 8080:8080 -n &lt;namespace&gt;\n   curl http://localhost:8080/metrics\n</code></pre></p> </li> <li> <p>Verify application logs: <pre><code>   kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#11-pricing-and-licensing","title":"11. Pricing and Licensing","text":"<ul> <li>Free Usage: Helm chart available for free.</li> <li>Annual Support Licensing: \u20b91,000 INR/year.</li> <li>Includes email support and regular updates.</li> </ul>"},{"location":"troubleshooting/#connect","title":"Connect","text":"<ul> <li>karti479@gmail.com</li> <li>https://www.linkedin.com/in/product-kartik/</li> </ul> \ud83d\udcde Contact Me <p> Email: karti479@gmail.com |      Phone: +91-8742983860 GitHub: GitHub Profile |      LinkedIn: LinkedIn Profile"}]}